# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H6fwzSww2VfZjTB6P78sBSMC1mXojcRM
"""

# Install necessary libraries
!pip install mediapipe opencv-python-headless pykalman

import cv2
import numpy as np
import mediapipe as mp
from scipy.ndimage import gaussian_filter
from pykalman import KalmanFilter

# Set up MediaPipe Pose estimator
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)

# Dictionary to store Kalman filters and their states for each joint
kalman_filters = {}
filtered_state_means = {}
filtered_state_covariances = {}

# Initialize a Kalman filter for each joint
def initialize_kalman_filters():
    for joint in range(33):  # MediaPipe has 33 pose landmarks
        kf = KalmanFilter(initial_state_mean=[0, 0], n_dim_obs=2)
        kf = kf.em(np.array([[0, 0]] * 20), n_iter=5)  # Initial setup with dummy data
        kalman_filters[joint] = kf
        # Initialize state means and covariances
        filtered_state_means[joint] = np.array([0, 0])
        filtered_state_covariances[joint] = np.eye(2)

initialize_kalman_filters()

def preprocess_frame(frame):
    # Convert to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Apply Gaussian filter for noise reduction
    gray = gaussian_filter(gray, sigma=1)

    # Adaptive background subtraction (optional)
    fg_mask = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                                    cv2.THRESH_BINARY, 11, 2)
    return fg_mask

def process_frame_with_mediapipe(frame):
    # Preprocess the frame
    fg_frame = preprocess_frame(frame)

    # Convert to RGB as required by MediaPipe
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Run MediaPipe pose estimation
    results = pose.process(rgb_frame)

    # Extract keypoints and apply Kalman filter for stability
    keypoints = []
    if results.pose_landmarks:
        for i, landmark in enumerate(results.pose_landmarks.landmark):
            x, y = int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])

            # Apply Kalman filter to each joint
            kf = kalman_filters[i]
            filtered_state_means[i], filtered_state_covariances[i] = kf.filter_update(
                filtered_state_means[i],
                filtered_state_covariances[i],
                np.array([x, y])
            )
            # Use the Kalman-filtered coordinates for the keypoints
            keypoints.append((int(filtered_state_means[i][0]), int(filtered_state_means[i][1])))
    else:
        keypoints = [None] * 33  # If no landmarks detected, fill with None
    return keypoints

def draw_keypoints(frame, keypoints):
    for joint in keypoints:
        if joint:
            cv2.circle(frame, (int(joint[0]), int(joint[1])), 5, (0, 255, 0), -1)
    return frame

from google.colab.patches import cv2_imshow

# Path to the input video
video_path = '/content/U12ntitled video - Made with Clipchamp123.mp4'  # Replace with your video path

cap = cv2.VideoCapture(video_path)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Process the current frame for pose estimation and tracking
    keypoints = process_frame_with_mediapipe(frame)

    # Draw keypoints on the frame
    frame = draw_keypoints(frame, keypoints)

    # Display the processed frame
    cv2_imshow(frame)  # Use cv2_imshow() for Colab
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()